{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Assignment - Tommaso Pessina - 961739\n",
    "\n",
    "The aim of this assignment is to analyse the first thrilogy of the Star Wars saga and, since this would not be relevant in the classifican case, we will create a SMS Spam/Ham classifier.\n",
    "\n",
    "First we should import the necessary package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From \"nltk\" download the necessary packages. NLTK is the most used (and suitable) package for TextMining or Natural Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the three Star Wars script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIV = open('SW_EpisodeIVclean.txt', 'r')\n",
    "LinesIV = fileIV.readlines()\n",
    "fileV = open('SW_EpisodeV.txt', 'r')\n",
    "LinesV = fileV.readlines()\n",
    "fileVI = open('SW_EpisodeVI.txt', 'r')\n",
    "LinesVI = fileVI.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're doomed!\\t\\t\\t\\t\\t\\t\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesIV[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \"english\" as language in order to get the english stopword and the punctuation for delete it from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = stopwords.words(\"english\") \n",
    "\n",
    "stop_words_en = stop_words_en + list(string.punctuation)\n",
    "#stop_words_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each word of each line (for each text) and delete the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1IV = []#list()\n",
    "text1V = []#list()\n",
    "text1VI = []#list()\n",
    "\n",
    "\n",
    "for line in LinesIV:\n",
    "    text1IV.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesV:\n",
    "    text1V.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesVI:\n",
    "    text1VI.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will delete the \" and `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textIV =[] #list()\n",
    "textV =[] #list()\n",
    "textVI =[] # list()\n",
    "\n",
    "for line in text1IV:\n",
    "    textIV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1V:\n",
    "    textV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1VI:\n",
    "    textVI.append([w for w in line if w not in [\"''\",'``']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", 'escape', 'princess', 'time']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textIV[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the list of sentences in a list of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textIV) for i in b]\n",
    "new_lV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textV) for i in b]\n",
    "new_lVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textVI) for i in b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should delete the digits at the start of each senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = str(new_lIV)\n",
    "l5 = str(new_lV)\n",
    "l6 = str(new_lVI)\n",
    "digit4 = re.findall(r'\\d+', l4)\n",
    "digit5 = re.findall(r'\\d+', l5)\n",
    "digit6 = re.findall(r'\\d+', l6)\n",
    "\n",
    "corpus4 =[] #list()\n",
    "for line in textIV:\n",
    "    corpus4.append([w for w in line if w not in digit4])\n",
    "    \n",
    "corpus5 =[] #list()\n",
    "for line in textV:\n",
    "    corpus5.append([w for w in line if w not in digit5])\n",
    "    \n",
    "corpus6 = [] #list()\n",
    "for line in textVI:\n",
    "    corpus6.append([w for w in line if w not in digit6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['traced', 'rebel', 'spies', 'link', 'find', 'secret', 'base']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus4[46]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the list of sentences in a list of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus4) for i in b]\n",
    "\n",
    "StarWarsV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus5) for i in b]\n",
    "\n",
    "StarWarsVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus6) for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "Now we will create bigrams, that are n-grams with n equals to two. They are group of two adjacency word and can be usefull in order to analyse which couple are more frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsIV = nltk.ngrams(StarWarsIV, n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsV = nltk.ngrams(StarWarsV, n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsVI = nltk.ngrams(StarWarsVI, n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in bigramsIV: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in bigramsV: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in bigramsVI: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency distribution\n",
    "Firstly we will create the frequency distribution for the bi-grams previously created in order to see if there are relevaent couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('ca', \"n't\"): 29, (\"'ve\", 'got'): 20, (\"'re\", 'going'): 19, (\"n't\", 'know'): 14, (\"'m\", 'going'): 13, ('let', \"'s\"): 12, ('luke', 'luke'): 12, ('obi-wan', 'kenobi'): 12, ('r2', 'unit'): 11, ('rebel', 'base'): 11, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams_IV = nltk.FreqDist(bigramsIV)\n",
    "freq_bigrams_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('lord', 'vader'): 20, ('...', 'han'): 16, ('han', \"'s\"): 15, ('ca', \"n't\"): 15, ('threepio', 'oh'): 15, (\"n't\", 'know'): 13, (\"'m\", 'going'): 12, ('han', 'well'): 11, ('threepio', 'sir'): 11, (\"'ve\", 'got'): 10, ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams_V = nltk.FreqDist(bigramsV)\n",
    "freq_bigrams_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('threepio', 'oh'): 20, ('master', 'luke'): 16, ('ca', \"n't\"): 15, ('dark', 'side'): 12, ('death', 'star'): 11, ('let', \"'s\"): 11, ('luke', \"'s\"): 11, ('gon', 'na'): 11, ('wo', \"n't\"): 9, ('threepio', 'artoo'): 8, ...})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams_VI = nltk.FreqDist(bigramsVI)\n",
    "freq_bigrams_VI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each movie, we will count the frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"'s\": 208, \"n't\": 175, '...': 145, \"'re\": 103, \"'ll\": 75, \"'m\": 75, 'going': 71, \"'ve\": 67, 'luke': 64, 'get': 61, ...})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqIV = nltk.FreqDist(StarWarsIV)\n",
    "freqIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'han': 196, 'luke': 157, 'leia': 130, \"'s\": 128, \"n't\": 118, '...': 116, 'threepio': 96, 'vader': 81, 'lando': 79, \"'re\": 63, ...})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqV = nltk.FreqDist(StarWarsV)\n",
    "freqV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'luke': 156, 'han': 133, \"'s\": 97, \"n't\": 96, 'threepio': 94, '...': 87, 'leia': 72, 'vader': 61, 'emperor': 60, 'lando': 48, ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqVI = nltk.FreqDist(StarWarsVI)\n",
    "freqVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will select the 10th most common word for each movie and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "swIV = collections.Counter(freqIV).most_common(10)\n",
    "swV = collections.Counter(freqV).most_common(10)\n",
    "swVI = collections.Counter(freqVI).most_common(10)\n",
    "\n",
    "swIV_bigrams = collections.Counter(freq_bigrams_IV).most_common(10)\n",
    "swV_bigrams = collections.Counter(freq_bigrams_V).most_common(10)\n",
    "swVI_bigrams = collections.Counter(freq_bigrams_VI).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a data frame whose column are the movies and the rows are the word-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list()\n",
    "test2 = list()\n",
    "test3 = list()\n",
    "testA = list()\n",
    "test2B = list()\n",
    "test3C = list()\n",
    "for elem in swIV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test.append(s)\n",
    "for elem in swV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test2.append(s)\n",
    "for elem in swVI:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test3.append(s)\n",
    "for elem in swIV_bigrams:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    testA.append(s)\n",
    "for elem in swV_bigrams:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test2B.append(s)\n",
    "for elem in swVI_bigrams:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test3C.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'swIV': test, 'swV': test2, \"swVI\":test3,'swIVbigrams': testA, 'swVbigrams': test2B, \"swVIbigrams\":test3C}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swIV</th>\n",
       "      <th>swV</th>\n",
       "      <th>swVI</th>\n",
       "      <th>swIVbigrams</th>\n",
       "      <th>swVbigrams</th>\n",
       "      <th>swVIbigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s - 208</td>\n",
       "      <td>han - 196</td>\n",
       "      <td>luke - 156</td>\n",
       "      <td>('ca', \"n't\") - 29</td>\n",
       "      <td>('lord', 'vader') - 20</td>\n",
       "      <td>('threepio', 'oh') - 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n't - 175</td>\n",
       "      <td>luke - 157</td>\n",
       "      <td>han - 133</td>\n",
       "      <td>(\"'ve\", 'got') - 20</td>\n",
       "      <td>('...', 'han') - 16</td>\n",
       "      <td>('master', 'luke') - 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... - 145</td>\n",
       "      <td>leia - 130</td>\n",
       "      <td>'s - 97</td>\n",
       "      <td>(\"'re\", 'going') - 19</td>\n",
       "      <td>('han', \"'s\") - 15</td>\n",
       "      <td>('ca', \"n't\") - 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'re - 103</td>\n",
       "      <td>'s - 128</td>\n",
       "      <td>n't - 96</td>\n",
       "      <td>(\"n't\", 'know') - 14</td>\n",
       "      <td>('ca', \"n't\") - 15</td>\n",
       "      <td>('dark', 'side') - 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'ll - 75</td>\n",
       "      <td>n't - 118</td>\n",
       "      <td>threepio - 94</td>\n",
       "      <td>(\"'m\", 'going') - 13</td>\n",
       "      <td>('threepio', 'oh') - 15</td>\n",
       "      <td>('death', 'star') - 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'m - 75</td>\n",
       "      <td>... - 116</td>\n",
       "      <td>... - 87</td>\n",
       "      <td>('let', \"'s\") - 12</td>\n",
       "      <td>(\"n't\", 'know') - 13</td>\n",
       "      <td>('let', \"'s\") - 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>going - 71</td>\n",
       "      <td>threepio - 96</td>\n",
       "      <td>leia - 72</td>\n",
       "      <td>('luke', 'luke') - 12</td>\n",
       "      <td>(\"'m\", 'going') - 12</td>\n",
       "      <td>('luke', \"'s\") - 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'ve - 67</td>\n",
       "      <td>vader - 81</td>\n",
       "      <td>vader - 61</td>\n",
       "      <td>('obi-wan', 'kenobi') - 12</td>\n",
       "      <td>('han', 'well') - 11</td>\n",
       "      <td>('gon', 'na') - 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>luke - 64</td>\n",
       "      <td>lando - 79</td>\n",
       "      <td>emperor - 60</td>\n",
       "      <td>('r2', 'unit') - 11</td>\n",
       "      <td>('threepio', 'sir') - 11</td>\n",
       "      <td>('wo', \"n't\") - 9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get - 61</td>\n",
       "      <td>'re - 63</td>\n",
       "      <td>lando - 48</td>\n",
       "      <td>('rebel', 'base') - 11</td>\n",
       "      <td>(\"'ve\", 'got') - 10</td>\n",
       "      <td>('threepio', 'artoo') - 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         swIV            swV           swVI                 swIVbigrams  \\\n",
       "0    's - 208      han - 196     luke - 156          ('ca', \"n't\") - 29   \n",
       "1   n't - 175     luke - 157      han - 133         (\"'ve\", 'got') - 20   \n",
       "2   ... - 145     leia - 130        's - 97       (\"'re\", 'going') - 19   \n",
       "3   're - 103       's - 128       n't - 96        (\"n't\", 'know') - 14   \n",
       "4    'll - 75      n't - 118  threepio - 94        (\"'m\", 'going') - 13   \n",
       "5     'm - 75      ... - 116       ... - 87          ('let', \"'s\") - 12   \n",
       "6  going - 71  threepio - 96      leia - 72       ('luke', 'luke') - 12   \n",
       "7    've - 67     vader - 81     vader - 61  ('obi-wan', 'kenobi') - 12   \n",
       "8   luke - 64     lando - 79   emperor - 60         ('r2', 'unit') - 11   \n",
       "9    get - 61       're - 63     lando - 48      ('rebel', 'base') - 11   \n",
       "\n",
       "                 swVbigrams                swVIbigrams  \n",
       "0    ('lord', 'vader') - 20    ('threepio', 'oh') - 20  \n",
       "1       ('...', 'han') - 16    ('master', 'luke') - 16  \n",
       "2        ('han', \"'s\") - 15         ('ca', \"n't\") - 15  \n",
       "3        ('ca', \"n't\") - 15      ('dark', 'side') - 12  \n",
       "4   ('threepio', 'oh') - 15     ('death', 'star') - 11  \n",
       "5      (\"n't\", 'know') - 13         ('let', \"'s\") - 11  \n",
       "6      (\"'m\", 'going') - 12        ('luke', \"'s\") - 11  \n",
       "7      ('han', 'well') - 11         ('gon', 'na') - 11  \n",
       "8  ('threepio', 'sir') - 11          ('wo', \"n't\") - 9  \n",
       "9       (\"'ve\", 'got') - 10  ('threepio', 'artoo') - 8  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that some of the most common bigrams actually rapresent charachter of the movie and most of the time they don't match the single word frequency distribution. So basically by counting each word and by counting bigrams we will obtain two different answer to different question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization\n",
    "We choose to use the k-means cluster algorithm which will calculate the mean for each cluster and divide the observation in n cluster according to their distance (i.e. of each observation) from the mean of the clusters (i.e. it's centroid).  \n",
    "Note that here we will use the TF-IDF vectorisation method on the array of lines and only of the Star Wars IVth movie for semplicity. The TF-IDF will simplify the clusterization process beacuse we will obtain a list of token without any stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(LinesIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=4, n_init=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of code will print, for each cluster, the first 10 element which belong to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " come\n",
      " don\n",
      " sir\n",
      " red\n",
      " ve\n",
      " think\n",
      " ship\n",
      " oh\n",
      " know\n",
      " got\n",
      "Cluster 1:\n",
      " ll\n",
      " right\n",
      " worry\n",
      " cover\n",
      " got\n",
      " ve\n",
      " ship\n",
      " escape\n",
      " better\n",
      " time\n",
      "Cluster 2:\n",
      " luke\n",
      " going\n",
      " pull\n",
      " come\n",
      " force\n",
      " long\n",
      " know\n",
      " try\n",
      " just\n",
      " let\n",
      "Cluster 3:\n",
      " like\n",
      " wait\n",
      " don\n",
      " dead\n",
      " looks\n",
      " hit\n",
      " look\n",
      " minute\n",
      " want\n",
      " got\n",
      "\n",
      "\n",
      "Prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of code we can see that the word \"falcon\" (short for \"Milleniuim Falcon\") is in the same cluster as the word \"ship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"falcon\"])\n",
    "Z = vectorizer.transform([\"ship\"])\n",
    "prediction = model.predict(Y)\n",
    "prediction1 = model.predict(Z)\n",
    "print(prediction)\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word\n",
    "We can use the fetaure extraction method from sklearn in order to vectorize our text. Again for semplicity we run it only on the Star Wars IV movie. A bag of word is basically an array of unique words that are present in our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(StarWarsIV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the print above will be huge we can see a more informative things that is it's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 1641)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also, insted, use (unique) bigrams as word for our bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa twenty', 'ain tmy', 'alive will', 'all powerful', 'anyway luke', 'artoo detoo', 'astro droids', 'bulk cruisers', 'check out', 'damned fool', 'eight seven', 'eight two', 'first class', 'first mate', 'five seven', 'fleet its', 'four one', 'freighter tanker', 'half sized', 'here it', 'human cyborg', 'it just', 'large scale', 'navi computer', 'ninety four', 'obi wan', 'oh five', 'oh two', 'one eight', 'one five', 'one man', 'one one', 'one three', 'one two', 'ray shielded', 'see threepio', 'short circuited', 'six eight', 'sixty five', 'stand by', 'star pilot', 'station set', 'thing she', 'three eight', 'three seven', 'three two', 'together you', 'turbo lasers', 'twenty one', 'twenty three', 'twenty threedegrees', 'two hundred', 'two seven', 'two six', 'tx four', 'weak minded', 'xp 38', 'you remy']\n"
     ]
    }
   ],
   "source": [
    "# This time we use 2-grams:\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(StarWarsIV)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply vectorize our text using the TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X1 = vectorizer.fit_transform(StarWarsIV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for semplicity, we can analyse it's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6167, 1641)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model\n",
    "Topic model is a statistical method for discovering the abstract topics that occur in a documents. To do this we will use the package \"spacy\" particularly suitable in natural language processing or sentimenatl analysis. We will use LDA as topic model which is used to classify text in a document to a particular topic.\n",
    "First we need to lemmatize our word, stemm it, tokenize it and delete any stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to install gensim that is a library for unsupervised topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\tompe\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "with open('SW_EpisodeIVclean.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a dictionary which is a mapping between words and their integer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data) #StarWarsIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data] # counts the number of occurrences of each distinct word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our model, an LDA model, with five topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print each topic with its coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.025*\"going\" + 0.022*\"think\" + 0.016*\"blast\" + 0.014*\"standing\" + 0.011*\"anything\" + 0.010*\"friend\"')\n",
      "(1, '0.040*\"going\" + 0.015*\"alderaan\" + 0.015*\"kenobi\" + 0.011*\"droid\" + 0.011*\"talking\" + 0.011*\"little\"')\n",
      "(2, '0.020*\"artoo\" + 0.016*\"going\" + 0.014*\"thing\" + 0.014*\"force\" + 0.012*\"worry\" + 0.011*\"would\"')\n",
      "(3, '0.013*\"uncle\" + 0.012*\"power\" + 0.011*\"biggs\" + 0.010*\"rebel\" + 0.010*\"station\" + 0.010*\"watch\"')\n",
      "(4, '0.055*\"right\" + 0.023*\"threepio\" + 0.017*\"leader\" + 0.015*\"think\" + 0.014*\"could\" + 0.013*\"father\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the same model but with ten topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.053*\"think\" + 0.032*\"leader\" + 0.021*\"blast\" + 0.020*\"chewie\" + 0.018*\"could\"')\n",
      "(1, '0.027*\"standing\" + 0.024*\"rebel\" + 0.022*\"worry\" + 0.018*\"think\" + 0.015*\"range\"')\n",
      "(2, '0.044*\"force\" + 0.016*\"computer\" + 0.014*\"system\" + 0.012*\"could\" + 0.012*\"always\"')\n",
      "(3, '0.017*\"artoo\" + 0.015*\"clear\" + 0.015*\"dangerous\" + 0.012*\"station\" + 0.012*\"space\"')\n",
      "(4, '0.096*\"going\" + 0.072*\"right\" + 0.017*\"station\" + 0.017*\"fighter\" + 0.013*\"battle\"')\n",
      "(5, '0.017*\"think\" + 0.015*\"talking\" + 0.014*\"uncle\" + 0.014*\"thought\" + 0.013*\"little\"')\n",
      "(6, '0.035*\"threepio\" + 0.033*\"going\" + 0.024*\"princess\" + 0.024*\"coming\" + 0.021*\"droids\"')\n",
      "(7, '0.023*\"father\" + 0.016*\"detention\" + 0.015*\"three\" + 0.014*\"vader\" + 0.009*\"going\"')\n",
      "(8, '0.036*\"kenobi\" + 0.026*\"anything\" + 0.016*\"happen\" + 0.014*\"friend\" + 0.014*\"great\"')\n",
      "(9, '0.038*\"artoo\" + 0.022*\"would\" + 0.021*\"enough\" + 0.019*\"friend\" + 0.019*\"something\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyse the overall topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = ldamodel.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / 4\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation\n",
    "Now we will apply summarisation to the IVth Star Wars movie by imposing a maximum of 100 words, but first we need to parser our text (for completeness we read from the beginning the file for each chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will read it as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SW_EpisodeIVclean.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank\n",
    "TextRank is an algorithm based on PageRank and is usefull in order to understand the summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Parse text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the next portion of code we can know some \"imporant\" word in our text which can be usefull for the summarisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship - 6.504885004948206\n",
      "time - 6.1955890912561875\n",
      "Luke - 6.1608797582721975\n",
      "power - 5.805977498614775\n",
      "sir - 5.662529301043638\n",
      "station - 5.340396594552676\n",
      "systems - 3.989081986904476\n",
      "lot - 3.897116519200681\n",
      "father - 3.782256009413122\n",
      "Empire - 3.5678547061732533\n",
      "Alderaan - 3.451203956231256\n",
      "unit - 3.3384542258123715\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(data, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print our summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke, I'm not going to wait for the Empire to draft me into service.\n",
      "I know, but he's got enough vaporators going to make the place pay off.\n",
      "I think those new droids are going to work out fine.\n",
      "You know that little droid is going to cause me a lot of trouble.\n",
      "I think it is time we demonstrate the full power of this station.Set your course for Princess Leia's home planet of Alderaan.\n",
      "Look, going good against remotes is one thing.\n",
      "It's going to be like old times Luke.\n",
      "Luke, let me know when you're going in.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(data, word_count=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, despite the short lenght, we can found almost all the words returned by the TextRank method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now we will use a different dataset. The scope here is to define a classificaion method that is able to classify SMS as SPAM or not (HAM). The result we would have obtained from the previous dataset would not have been relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here is very important to delete any stopwords, convert all the words to lowercase and stemm it (i.e. remove the last part of the word in order to make it like the infinit form the verb). If we forgot to do this we may obtain a very low accuracy in the final model because, basically, it will result in a more complex task for the machine. Here we will use the porter stemmer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset['cleaned'] = dataset['Message'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will vectorize it by using the TF-IDF method by deleting any stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4118)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our model. Note that we choose the logistic regression method because, since dealing with text can be a complex problem for our machines, we prefer a simpler method. Logistic regression basically classify binary dependent variable.\n",
    "Last but not least, we print the confusion matrix for our model and we obtain a very accurate model, and we know that too accurate model need deeper analysis becuase they may suffer from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1447\n",
      "        spam       0.98      0.79      0.88       225\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.98      0.89      0.93      1672\n",
      "weighted avg       0.97      0.97      0.97      1672\n",
      "\n",
      "[[1444    3]\n",
      " [  47  178]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# FeatureUnion combines two or more pipelines or transformers\n",
    "# and is very fast!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = dataset['cleaned']\n",
    "Y = dataset['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer),\n",
    "                     ('chi',  SelectKBest(chi2, k=1200)),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "with open('LogisticRegression.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "\n",
    "# confusion matrix and classification report(precision, recall, F1-score)\n",
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Facts from text\n",
    "Now we can use the package \"spacy\" in order to extract fact from text. Here we will use again the Star Wars dataset. We know that, at the beginning of the IVth Star Wars movie, Darth Vader was in search of the Death Star plans aboard the Princess Leils's ship but cannot find them. Now, iff we extract facts about \"plans\", indeed we see that they are \"not aboard this ship!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_facts(text,keyword):\n",
    "    \n",
    "    # Parse the document with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract semi-structured statements\n",
    "    statements = textacy.extract.semistructured_statements(doc, keyword)\n",
    "\n",
    "    for statement in statements:\n",
    "        subject, verb, fact = statement\n",
    "        print(f\" - {fact}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - not in the main computer.\t\t\t\t\t\t\n",
      " - not aboard this ship!  \n",
      " - back in our hands.\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "print_facts(data,\"plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
