{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIV = open('SW_EpisodeIVclean.txt', 'r')\n",
    "LinesIV = fileIV.readlines()\n",
    "fileV = open('SW_EpisodeV.txt', 'r')\n",
    "LinesV = fileV.readlines()\n",
    "fileVI = open('SW_EpisodeVI.txt', 'r')\n",
    "LinesVI = fileVI.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're doomed!\\t\\t\\t\\t\\t\\t\\n\""
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesIV[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = stopwords.words(\"english\") \n",
    "\n",
    "stop_words_en = stop_words_en + list(string.punctuation)\n",
    "#stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1IV = []#list()\n",
    "text1V = []#list()\n",
    "text1VI = []#list()\n",
    "\n",
    "\n",
    "for line in LinesIV:\n",
    "    text1IV.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesV:\n",
    "    text1V.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesVI:\n",
    "    text1VI.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "textIV =[] #list()\n",
    "textV =[] #list()\n",
    "textVI =[] # list()\n",
    "\n",
    "for line in text1IV:\n",
    "    textIV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1V:\n",
    "    textV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1VI:\n",
    "    textVI.append([w for w in line if w not in [\"''\",'``']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", 'escape', 'princess', 'time']"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textIV[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textIV) for i in b]\n",
    "new_lV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textV) for i in b]\n",
    "new_lVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textVI) for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = str(new_lIV)\n",
    "l5 = str(new_lV)\n",
    "l6 = str(new_lVI)\n",
    "digit4 = re.findall(r'\\d+', l4)\n",
    "digit5 = re.findall(r'\\d+', l5)\n",
    "digit6 = re.findall(r'\\d+', l6)\n",
    "\n",
    "corpus4 =[] #list()\n",
    "for line in textIV:\n",
    "    corpus4.append([w for w in line if w not in digit4])\n",
    "    \n",
    "corpus5 =[] #list()\n",
    "for line in textV:\n",
    "    corpus5.append([w for w in line if w not in digit5])\n",
    "    \n",
    "corpus6 = [] #list()\n",
    "for line in textVI:\n",
    "    corpus6.append([w for w in line if w not in digit6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", 'escape', 'princess', 'time']"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus4[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus4) for i in b]\n",
    "\n",
    "StarWarsV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus5) for i in b]\n",
    "\n",
    "StarWarsVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus6) for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsIV, n = 4)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsV, n = 4)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsVI, n = 4)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"'s\": 208, \"n't\": 175, '...': 145, \"'re\": 103, \"'ll\": 75, \"'m\": 75, 'going': 71, \"'ve\": 67, 'luke': 64, 'get': 61, ...})"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqIV = nltk.FreqDist(StarWarsIV)\n",
    "freqIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'han': 196, 'luke': 157, 'leia': 130, \"'s\": 128, \"n't\": 118, '...': 116, 'threepio': 96, 'vader': 81, 'lando': 79, \"'re\": 63, ...})"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqV = nltk.FreqDist(StarWarsV)\n",
    "freqV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'luke': 156, 'han': 133, \"'s\": 97, \"n't\": 96, 'threepio': 94, '...': 87, 'leia': 72, 'vader': 61, 'emperor': 60, 'lando': 48, ...})"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqVI = nltk.FreqDist(StarWarsVI)\n",
    "freqVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "swIV = collections.Counter(freqIV).most_common(10)\n",
    "swV = collections.Counter(freqV).most_common(10)\n",
    "swVI = collections.Counter(freqVI).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list()\n",
    "test2 = list()\n",
    "test3 = list()\n",
    "for elem in swIV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test.append(s)\n",
    "for elem in swV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test2.append(s)\n",
    "for elem in swVI:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test3.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'swIV': test, 'swV': test2, \"swVI\":test3}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swIV</th>\n",
       "      <th>swV</th>\n",
       "      <th>swVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s - 208</td>\n",
       "      <td>han - 196</td>\n",
       "      <td>luke - 156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n't - 175</td>\n",
       "      <td>luke - 157</td>\n",
       "      <td>han - 133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... - 145</td>\n",
       "      <td>leia - 130</td>\n",
       "      <td>'s - 97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'re - 103</td>\n",
       "      <td>'s - 128</td>\n",
       "      <td>n't - 96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'ll - 75</td>\n",
       "      <td>n't - 118</td>\n",
       "      <td>threepio - 94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'m - 75</td>\n",
       "      <td>... - 116</td>\n",
       "      <td>... - 87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>going - 71</td>\n",
       "      <td>threepio - 96</td>\n",
       "      <td>leia - 72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'ve - 67</td>\n",
       "      <td>vader - 81</td>\n",
       "      <td>vader - 61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>luke - 64</td>\n",
       "      <td>lando - 79</td>\n",
       "      <td>emperor - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get - 61</td>\n",
       "      <td>'re - 63</td>\n",
       "      <td>lando - 48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         swIV            swV           swVI\n",
       "0    's - 208      han - 196     luke - 156\n",
       "1   n't - 175     luke - 157      han - 133\n",
       "2   ... - 145     leia - 130        's - 97\n",
       "3   're - 103       's - 128       n't - 96\n",
       "4    'll - 75      n't - 118  threepio - 94\n",
       "5     'm - 75      ... - 116       ... - 87\n",
       "6  going - 71  threepio - 96      leia - 72\n",
       "7    've - 67     vader - 81     vader - 61\n",
       "8   luke - 64     lando - 79   emperor - 60\n",
       "9    get - 61       're - 63     lando - 48"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(LinesIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=4, n_init=1)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " drop\n",
      " shipments\n",
      " smugglers\n",
      " cruiser\n",
      " sign\n",
      " jabba\n",
      " imperial\n",
      " time\n",
      " zone\n",
      " form\n",
      "Cluster 1:\n",
      " understand\n",
      " need\n",
      " luke\n",
      " zone\n",
      " fort\n",
      " foolish\n",
      " force\n",
      " forever\n",
      " forget\n",
      " forgot\n",
      "Cluster 2:\n",
      " forgot\n",
      " battle\n",
      " look\n",
      " come\n",
      " right\n",
      " going\n",
      " zone\n",
      " fortress\n",
      " force\n",
      " forever\n",
      "Cluster 3:\n",
      " going\n",
      " luke\n",
      " ll\n",
      " don\n",
      " come\n",
      " right\n",
      " sir\n",
      " ve\n",
      " red\n",
      " think\n",
      "\n",
      "\n",
      "Prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"ship\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(StarWarsIV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7313, 1665)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we use 2-grams:\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(StarWarsIV)\n",
    "print(vectorizer2.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X1 = vectorizer.fit_transform(StarWarsIV)\n",
    "#X2 = vectorizer.fit_transform(StarWarsV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7313, 1665)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Custom Transformer (Inheriting from classes)\n",
    "class CleanText( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    # The class constructor is formed by a function with double underscore __ :\n",
    "    # these are called 'special functions' as they have special meaning.\n",
    "    # In particular the '__init__' gets called whenever \n",
    "    # a new object of that class is instantiated,\n",
    "    # and are used to initialize all the necessary variables.\n",
    "    # In this example we initialize the language variable 'lang' with 'English'\n",
    "    # and pick the SnowballStemmer as the default stemmer.\n",
    "    def __init__( self, lang = \"english\"):\n",
    "        self.lang = lang\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "    \n",
    "    # The 'fit' method here is used to instantiate the class on the 'self' variable \n",
    "    # and return the object itself     \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    # Custom function: this applies the stemmer just created in the '__init__'\n",
    "    # part to the 'self' variable\n",
    "    def clean( self, x ):\n",
    "        words   = [self.stemmer.stem(word) for word in word_tokenize(x.lower()) if word.isalpha() and word not in stopwords.words(\"english\")]\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    # Method that describes what we need this transformer to do i.e. cleaning the text\n",
    "    # in the 'text' column in the data frame.\n",
    "    # This will be used later on in the usage of the custom transformer \n",
    "    # within the pipeline.\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[\"text\"].apply(self.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer: same parts as the previous custom transformer\n",
    "# This one will be used for feature extraction\n",
    "\n",
    "class CustomFeatures( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    def __init__( self ):\n",
    "        return\n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "        \n",
    "    # Method that describes what we need this transformer to do i.e.\n",
    "    # returning length, digits and punctuations in the 'text' column in data frame\n",
    "    def transform( self, X, y = None ):\n",
    "        f           = pd.DataFrame()\n",
    "        f['len']    = X['text'].str.len()\n",
    "        f['digits'] = X['text'].str.findall(r'\\d').str.len()\n",
    "        f['punct']  = X['text'].str.findall(r'[^a-zA-Z\\d\\s:]').str.len()\n",
    "        return f[['len','digits','punct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# FeatureUnion combines two or more pipelines or transformers\n",
    "# and is very fast!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Our first pipeline called 'pipe' will be formed by three 'steps' or parts:\n",
    "# 1)\"extract\" which in turns is formed through FeatureUnion which put together two parts:\n",
    "# \"terms\" (formed by a pipeline with the CleanText() transformer we created above\n",
    "# and the TfidVectorize text vectorizing transformer from scikit-learn) and \"custom\" \n",
    "# (formed by the CustomFeatures transformer we created above);\n",
    "# 2) \"select\", formed by the scikit-learn transformer method \"SelectKBest\" for feature \n",
    "# selection with a chi squared score function;\n",
    "# 3) \"scale\", same as 2) using the StandardScaler method from scikit-learn. \n",
    "# The whole pipeline will be used as pre-processing task in classifying pipelines.\n",
    "pipe = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                               ('tfidf', TfidfVectorizer())])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe_logistic = Pipeline([('pre_process', pipe),\n",
    "                          ('classify', LogisticRegression(max_iter=10000, tol=0.1, solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "pipe_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109632 sha256=de128c579dc253aa19021a04cdf120153fbb03918bd1b8d035f91d312ef07d89\n",
      "  Stored in directory: c:\\users\\tompe\\appdata\\local\\pip\\cache\\wheels\\24\\f6\\ea\\70a0761bdfaeacff66662751fe71920e25c4c43d97098a3886\n",
      "Successfully built smart-open\n",
      "Installing collected packages: Cython, smart-open, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "with open('SW_EpisodeIVclean.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data) #StarWarsIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.061*\"going\" + 0.024*\"could\" + 0.020*\"think\" + 0.013*\"uncle\"')\n",
      "(1, '0.023*\"coming\" + 0.017*\"standing\" + 0.017*\"fighter\" + 0.014*\"range\"')\n",
      "(2, '0.015*\"father\" + 0.011*\"listen\" + 0.009*\"found\" + 0.008*\"look\"')\n",
      "(3, '0.023*\"threepio\" + 0.020*\"artoo\" + 0.016*\"leader\" + 0.016*\"kenobi\"')\n",
      "(4, '0.035*\"right\" + 0.016*\"force\" + 0.014*\"would\" + 0.010*\"system\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"standing\" + 0.022*\"jabba\" + 0.015*\"nothing\" + 0.013*\"going\"')\n",
      "(1, '0.064*\"going\" + 0.042*\"think\" + 0.023*\"father\" + 0.022*\"force\"')\n",
      "(2, '0.023*\"leader\" + 0.016*\"rebellion\" + 0.016*\"princess\" + 0.013*\"hurry\"')\n",
      "(3, '0.021*\"biggs\" + 0.016*\"course\" + 0.014*\"blaster\" + 0.014*\"thing\"')\n",
      "(4, '0.059*\"artoo\" + 0.051*\"threepio\" + 0.027*\"droids\" + 0.021*\"great\"')\n",
      "(5, '0.033*\"friend\" + 0.025*\"coming\" + 0.019*\"anything\" + 0.019*\"little\"')\n",
      "(6, '0.014*\"computer\" + 0.014*\"would\" + 0.014*\"wrong\" + 0.011*\"somebody\"')\n",
      "(7, '0.027*\"better\" + 0.022*\"kenobi\" + 0.022*\"worry\" + 0.020*\"going\"')\n",
      "(8, '0.067*\"right\" + 0.026*\"fighter\" + 0.016*\"chewie\" + 0.016*\"range\"')\n",
      "(9, '0.041*\"could\" + 0.028*\"blast\" + 0.018*\"going\" + 0.018*\"happen\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SW_EpisodeIVclean.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke, I'm not going to wait for the Empire to draft me into service.\n",
      "I know, but he's got enough vaporators going to make the place pay off.\n",
      "I think those new droids are going to work out fine.\n",
      "You know that little droid is going to cause me a lot of trouble.\n",
      "I think it is time we demonstrate the full power of this station.Set your course for Princess Leia's home planet of Alderaan.\n",
      "Look, going good against remotes is one thing.\n",
      "It's going to be like old times Luke.\n",
      "Luke, let me know when you're going in.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(data, word_count=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Parse text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship - 6.504885004948206\n",
      "time - 6.1955890912561875\n",
      "Luke - 6.1608797582721975\n",
      "power - 5.805977498614775\n",
      "sir - 5.662529301043638\n",
      "station - 5.340396594552676\n",
      "systems - 3.989081986904476\n",
      "lot - 3.897116519200681\n",
      "father - 3.782256009413122\n",
      "Empire - 3.5678547061732533\n",
      "Alderaan - 3.451203956231256\n",
      "unit - 3.3384542258123715\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(data, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset['cleaned'] = dataset['Message'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4118)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1228\n",
      "        spam       0.99      0.82      0.90       165\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.91      0.94      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1226    2]\n",
      " [  29  136]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "X = dataset['cleaned']\n",
    "Y = dataset['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer),\n",
    "                     ('chi',  SelectKBest(chi2, k=1200)),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "with open('LogisticRegression.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "\n",
    "# confusion matrix and classification report(precision, recall, F1-score)\n",
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
