{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Assignment - Tommaso Pessina\n",
    "\n",
    "The aim of this assignment is to analyse the first thrilogy of the Star Wars saga and, since this would not be relevant in the classifican case, we will create a SMS Spam/Ham classifier.\n",
    "\n",
    "First we should import the necessary package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From \"nltk\" download the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the three Star Wars script file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIV = open('SW_EpisodeIVclean.txt', 'r')\n",
    "LinesIV = fileIV.readlines()\n",
    "fileV = open('SW_EpisodeV.txt', 'r')\n",
    "LinesV = fileV.readlines()\n",
    "fileVI = open('SW_EpisodeVI.txt', 'r')\n",
    "LinesVI = fileVI.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're doomed!\\t\\t\\t\\t\\t\\t\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesIV[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set \"english\" as language in order to get the english stopword for delete it from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = stopwords.words(\"english\") \n",
    "\n",
    "stop_words_en = stop_words_en + list(string.punctuation)\n",
    "#stop_words_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize each word of each line (for each text) and delete the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1IV = []#list()\n",
    "text1V = []#list()\n",
    "text1VI = []#list()\n",
    "\n",
    "\n",
    "for line in LinesIV:\n",
    "    text1IV.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesV:\n",
    "    text1V.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])\n",
    "for line in LinesVI:\n",
    "    text1VI.append([w for w in nltk.word_tokenize(line.lower()) if w not in stop_words_en])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will delete the \" and `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "textIV =[] #list()\n",
    "textV =[] #list()\n",
    "textVI =[] # list()\n",
    "\n",
    "for line in text1IV:\n",
    "    textIV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1V:\n",
    "    textV.append([w for w in line if w not in [\"''\",'``']])\n",
    "for line in text1VI:\n",
    "    textVI.append([w for w in line if w not in [\"''\",'``']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", 'escape', 'princess', 'time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textIV[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the list of sentences in a list of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textIV) for i in b]\n",
    "new_lV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textV) for i in b]\n",
    "new_lVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, textVI) for i in b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should delete the digits at the start of each senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = str(new_lIV)\n",
    "l5 = str(new_lV)\n",
    "l6 = str(new_lVI)\n",
    "digit4 = re.findall(r'\\d+', l4)\n",
    "digit5 = re.findall(r'\\d+', l5)\n",
    "digit6 = re.findall(r'\\d+', l6)\n",
    "\n",
    "corpus4 =[] #list()\n",
    "for line in textIV:\n",
    "    corpus4.append([w for w in line if w not in digit4])\n",
    "    \n",
    "corpus5 =[] #list()\n",
    "for line in textV:\n",
    "    corpus5.append([w for w in line if w not in digit5])\n",
    "    \n",
    "corpus6 = [] #list()\n",
    "for line in textVI:\n",
    "    corpus6.append([w for w in line if w not in digit6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", 'escape', 'princess', 'time']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus4[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the list of sentences in a list of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus4) for i in b]\n",
    "\n",
    "StarWarsV = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus5) for i in b]\n",
    "\n",
    "StarWarsVI = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, corpus6) for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StarWarsIV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "Now we will create bigrams that are group of two word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsIV, n = 2)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsV, n = 2)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(StarWarsVI, n = 2)\n",
    "for grams in bigrams: \n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distribution\n",
    "For each movie count the frequency of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"'s\": 208, \"n't\": 175, '...': 145, \"'re\": 103, \"'ll\": 75, \"'m\": 75, 'going': 71, \"'ve\": 67, 'luke': 64, 'get': 61, ...})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqIV = nltk.FreqDist(StarWarsIV)\n",
    "freqIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'han': 196, 'luke': 157, 'leia': 130, \"'s\": 128, \"n't\": 118, '...': 116, 'threepio': 96, 'vader': 81, 'lando': 79, \"'re\": 63, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqV = nltk.FreqDist(StarWarsV)\n",
    "freqV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'luke': 156, 'han': 133, \"'s\": 97, \"n't\": 96, 'threepio': 94, '...': 87, 'leia': 72, 'vader': 61, 'emperor': 60, 'lando': 48, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqVI = nltk.FreqDist(StarWarsVI)\n",
    "freqVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will select the 10th most common word for each movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "swIV = collections.Counter(freqIV).most_common(10)\n",
    "swV = collections.Counter(freqV).most_common(10)\n",
    "swVI = collections.Counter(freqVI).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a data frame whose column are the movies and the rows are the word-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list()\n",
    "test2 = list()\n",
    "test3 = list()\n",
    "for elem in swIV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test.append(s)\n",
    "for elem in swV:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test2.append(s)\n",
    "for elem in swVI:\n",
    "    s = \"{} - {}\".format(elem[0], elem[1])\n",
    "    test3.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'swIV': test, 'swV': test2, \"swVI\":test3}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>swIV</th>\n",
       "      <th>swV</th>\n",
       "      <th>swVI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s - 208</td>\n",
       "      <td>han - 196</td>\n",
       "      <td>luke - 156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n't - 175</td>\n",
       "      <td>luke - 157</td>\n",
       "      <td>han - 133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... - 145</td>\n",
       "      <td>leia - 130</td>\n",
       "      <td>'s - 97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'re - 103</td>\n",
       "      <td>'s - 128</td>\n",
       "      <td>n't - 96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'ll - 75</td>\n",
       "      <td>n't - 118</td>\n",
       "      <td>threepio - 94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'m - 75</td>\n",
       "      <td>... - 116</td>\n",
       "      <td>... - 87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>going - 71</td>\n",
       "      <td>threepio - 96</td>\n",
       "      <td>leia - 72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'ve - 67</td>\n",
       "      <td>vader - 81</td>\n",
       "      <td>vader - 61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>luke - 64</td>\n",
       "      <td>lando - 79</td>\n",
       "      <td>emperor - 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get - 61</td>\n",
       "      <td>'re - 63</td>\n",
       "      <td>lando - 48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         swIV            swV           swVI\n",
       "0    's - 208      han - 196     luke - 156\n",
       "1   n't - 175     luke - 157      han - 133\n",
       "2   ... - 145     leia - 130        's - 97\n",
       "3   're - 103       's - 128       n't - 96\n",
       "4    'll - 75      n't - 118  threepio - 94\n",
       "5     'm - 75      ... - 116       ... - 87\n",
       "6  going - 71  threepio - 96      leia - 72\n",
       "7    've - 67     vader - 81     vader - 61\n",
       "8   luke - 64     lando - 79   emperor - 60\n",
       "9    get - 61       're - 63     lando - 48"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization\n",
    "We choose to use the k-means cluster algorithm which will calculate the mean for each cluster and divide the observation in n cluster according to their distance (i.e. of each observation) from the mean of the clusters.  \n",
    "Note that here we will use the TF-IDF vectorisation method on the array of lines only of the Star Wars IVth movie for semplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(LinesIV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=4, n_init=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of code will print, for each cluster, the first 10 element which belong to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " guard\n",
      " follow\n",
      " stand\n",
      " zone\n",
      " fort\n",
      " force\n",
      " forever\n",
      " forget\n",
      " forgot\n",
      " forgotten\n",
      "Cluster 1:\n",
      " kill\n",
      " going\n",
      " girl\n",
      " wonderful\n",
      " beginning\n",
      " like\n",
      " zone\n",
      " forms\n",
      " force\n",
      " forever\n",
      "Cluster 2:\n",
      " luke\n",
      " going\n",
      " don\n",
      " come\n",
      " sir\n",
      " ll\n",
      " ve\n",
      " red\n",
      " think\n",
      " know\n",
      "Cluster 3:\n",
      " right\n",
      " ll\n",
      " great\n",
      " boss\n",
      " wrong\n",
      " stand\n",
      " threepio\n",
      " check\n",
      " ship\n",
      " shut\n",
      "\n",
      "\n",
      "Prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of code we can see that \"falcon\", short for \"Milleniuim Falcon\" (i.e. Han Solo's ship), is in the same cluster as the word \"ship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"falcon\"])\n",
    "Z = vectorizer.transform([\"ship\"])\n",
    "prediction = model.predict(Y)\n",
    "prediction1 = model.predict(Z)\n",
    "print(prediction)\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of word\n",
    "We can use the fetaure extraction method from sklearn in order to vectorize our text. Again for semplicity we run it only on the Star Wars IV movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(StarWarsIV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7313, 1665)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we use 2-grams:\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(StarWarsIV)\n",
    "print(vectorizer2.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply vectorize our text using the TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X1 = vectorizer.fit_transform(StarWarsIV)\n",
    "#X2 = vectorizer.fit_transform(StarWarsV)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7313, 1665)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Custom Transformer (Inheriting from classes)\n",
    "class CleanText( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    # The class constructor is formed by a function with double underscore __ :\n",
    "    # these are called 'special functions' as they have special meaning.\n",
    "    # In particular the '__init__' gets called whenever \n",
    "    # a new object of that class is instantiated,\n",
    "    # and are used to initialize all the necessary variables.\n",
    "    # In this example we initialize the language variable 'lang' with 'English'\n",
    "    # and pick the SnowballStemmer as the default stemmer.\n",
    "    def __init__( self, lang = \"english\"):\n",
    "        self.lang = lang\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "    \n",
    "    # The 'fit' method here is used to instantiate the class on the 'self' variable \n",
    "    # and return the object itself     \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "    \n",
    "    # Custom function: this applies the stemmer just created in the '__init__'\n",
    "    # part to the 'self' variable\n",
    "    def clean( self, x ):\n",
    "        words   = [self.stemmer.stem(word) for word in word_tokenize(x.lower()) if word.isalpha() and word not in stopwords.words(\"english\")]\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    # Method that describes what we need this transformer to do i.e. cleaning the text\n",
    "    # in the 'text' column in the data frame.\n",
    "    # This will be used later on in the usage of the custom transformer \n",
    "    # within the pipeline.\n",
    "    def transform( self, X, y = None ):\n",
    "        return X[\"text\"].apply(self.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer: same parts as the previous custom transformer\n",
    "# This one will be used for feature extraction\n",
    "\n",
    "class CustomFeatures( BaseEstimator, TransformerMixin ):\n",
    "    \n",
    "    # Class Constructor \n",
    "    def __init__( self ):\n",
    "        return\n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit( self, X, y = None ):\n",
    "        return self \n",
    "        \n",
    "    # Method that describes what we need this transformer to do i.e.\n",
    "    # returning length, digits and punctuations in the 'text' column in data frame\n",
    "    def transform( self, X, y = None ):\n",
    "        f           = pd.DataFrame()\n",
    "        f['len']    = X['text'].str.len()\n",
    "        f['digits'] = X['text'].str.findall(r'\\d').str.len()\n",
    "        f['punct']  = X['text'].str.findall(r'[^a-zA-Z\\d\\s:]').str.len()\n",
    "        return f[['len','digits','punct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# FeatureUnion combines two or more pipelines or transformers\n",
    "# and is very fast!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Our first pipeline called 'pipe' will be formed by three 'steps' or parts:\n",
    "# 1)\"extract\" which in turns is formed through FeatureUnion which put together two parts:\n",
    "# \"terms\" (formed by a pipeline with the CleanText() transformer we created above\n",
    "# and the TfidVectorize text vectorizing transformer from scikit-learn) and \"custom\" \n",
    "# (formed by the CustomFeatures transformer we created above);\n",
    "# 2) \"select\", formed by the scikit-learn transformer method \"SelectKBest\" for feature \n",
    "# selection with a chi squared score function;\n",
    "# 3) \"scale\", same as 2) using the StandardScaler method from scikit-learn. \n",
    "# The whole pipeline will be used as pre-processing task in classifying pipelines.\n",
    "pipe = Pipeline([(\"extract\", FeatureUnion([(\"terms\", Pipeline([('clean', CleanText()), \n",
    "                                                               ('tfidf', TfidfVectorizer())])),\n",
    "                                           (\"custom\", CustomFeatures())])),\n",
    "                 (\"select\", SelectKBest(score_func = chi2)),\n",
    "                 (\"scale\", StandardScaler(with_mean = False))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe_logistic = Pipeline([('pre_process', pipe),\n",
    "                          ('classify', LogisticRegression(max_iter=10000, tol=0.1, solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training\n",
    "pipe_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model\n",
    "Topic model is a statistical method for discovering the abstract topics that occur in a documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\tompe\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\tompe\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "with open('SW_EpisodeIVclean.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data) #StarWarsIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.056*\"going\" + 0.025*\"think\" + 0.014*\"leader\" + 0.013*\"standing\"')\n",
      "(1, '0.016*\"could\" + 0.015*\"would\" + 0.013*\"droid\" + 0.013*\"coming\"')\n",
      "(2, '0.030*\"right\" + 0.015*\"father\" + 0.014*\"going\" + 0.013*\"uncle\"')\n",
      "(3, '0.022*\"threepio\" + 0.017*\"force\" + 0.016*\"imperial\" + 0.013*\"talking\"')\n",
      "(4, '0.029*\"artoo\" + 0.015*\"blast\" + 0.011*\"think\" + 0.011*\"droids\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.017*\"anything\" + 0.017*\"something\" + 0.016*\"escape\" + 0.014*\"battle\"')\n",
      "(1, '0.093*\"going\" + 0.025*\"standing\" + 0.022*\"thing\" + 0.020*\"computer\"')\n",
      "(2, '0.041*\"could\" + 0.028*\"kenobi\" + 0.019*\"enough\" + 0.014*\"planet\"')\n",
      "(3, '0.029*\"would\" + 0.026*\"droids\" + 0.018*\"attack\" + 0.015*\"battle\"')\n",
      "(4, '0.047*\"threepio\" + 0.017*\"found\" + 0.016*\"stand\" + 0.014*\"better\"')\n",
      "(5, '0.040*\"artoo\" + 0.015*\"point\" + 0.013*\"seven\" + 0.013*\"destroy\"')\n",
      "(6, '0.085*\"right\" + 0.042*\"think\" + 0.029*\"blast\" + 0.016*\"hurry\"')\n",
      "(7, '0.025*\"fighter\" + 0.023*\"leader\" + 0.019*\"range\" + 0.016*\"talking\"')\n",
      "(8, '0.026*\"biggs\" + 0.019*\"going\" + 0.015*\"rebel\" + 0.013*\"wedge\"')\n",
      "(9, '0.037*\"alderaan\" + 0.025*\"force\" + 0.023*\"coming\" + 0.023*\"uncle\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation\n",
    "Now we will apply summarisation to the IVth Star Wars movie by imposing a maximum of 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SW_EpisodeIVclean.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke, I'm not going to wait for the Empire to draft me into service.\n",
      "I know, but he's got enough vaporators going to make the place pay off.\n",
      "I think those new droids are going to work out fine.\n",
      "You know that little droid is going to cause me a lot of trouble.\n",
      "I think it is time we demonstrate the full power of this station.Set your course for Princess Leia's home planet of Alderaan.\n",
      "Look, going good against remotes is one thing.\n",
      "It's going to be like old times Luke.\n",
      "Luke, let me know when you're going in.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(data, word_count=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Parse text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship - 6.504885004948206\n",
      "time - 6.1955890912561875\n",
      "Luke - 6.1608797582721975\n",
      "power - 5.805977498614775\n",
      "sir - 5.662529301043638\n",
      "station - 5.340396594552676\n",
      "systems - 3.989081986904476\n",
      "lot - 3.897116519200681\n",
      "father - 3.782256009413122\n",
      "Empire - 3.5678547061732533\n",
      "Alderaan - 3.451203956231256\n",
      "unit - 3.3384542258123715\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(data, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now we will use a different dataset. The scope here is to define a classificaion method that is able to classify SMS as SPAM or not (HAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here is very important to delete any stopwords, convert all the words to lowercase and stemm it (i.e. remove the last part of the word in order to make it like the infinit form the verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset['cleaned'] = dataset['Message'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will vectorize it by using the TF-IDF method by deleting any stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4118)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our model. Note that we choose the logistic regression method because since dealing with text is a complex problem we prefer a simpler method. Logistic regression basically classify binay dependent variable.\n",
    "Last but not least, we print the confusion matrix for our model and we obtain a very accurate model, maybe too accurate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      1214\n",
      "        spam       0.99      0.80      0.89       179\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.90      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "[[1213    1]\n",
      " [  35  144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# FeatureUnion combines two or more pipelines or transformers\n",
    "# and is very fast!\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = dataset['cleaned']\n",
    "Y = dataset['Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "pipeline = Pipeline([('vect', vectorizer),\n",
    "                     ('chi',  SelectKBest(chi2, k=1200)),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "with open('LogisticRegression.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "\n",
    "# confusion matrix and classification report(precision, recall, F1-score)\n",
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Facts from text\n",
    "Now we use the package \"spacy\" in order to extract fact from text. We know that at the start of the IVth Star Wars movie Darth Vader was in search of the Death Star plans and if we extract facts about \"plans\" indeed we see that they are \"not aboard this ship!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_facts(text,keyword):\n",
    "    \n",
    "    # Parse the document with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract semi-structured statements\n",
    "    statements = textacy.extract.semistructured_statements(doc, keyword)\n",
    "\n",
    "    for statement in statements:\n",
    "        subject, verb, fact = statement\n",
    "        print(f\" - {fact}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - not in the main computer.\t\t\t\t\t\t\n",
      " - not aboard this ship!  \n",
      " - back in our hands.\t\t\t\t\t\t\n"
     ]
    }
   ],
   "source": [
    "print_facts(data,\"plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
